{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWFyBEeNQxeA2Wow7u+RPg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HafizMuhammadAnas/text_summary/blob/main/Text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, you need to install the necessary libraries.\n",
        "# Run this cell to install the required packages.\n",
        "!pip install openai langchain tiktoken pypdf unstructured pandas chromadb FPDF"
      ],
      "metadata": {
        "id": "x8kXusZaFPOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries and set your OpenAI API key.\n",
        "import langchain\n",
        "from langchain import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "import os\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "COWxv0SfFQ7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your OpenAI API key here.\n",
        "openai_api_key = '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'"
      ],
      "metadata": {
        "id": "3EVYS7wnFQ-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the OpenAI model and load the PDF file.\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "# Load the book from the PDF file (replace \"crime-and-punishment.pdf\" with your file).\n",
        "loader = PyPDFLoader(\"crime-and-punishment.pdf\")\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "vIHq1MrEFRBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the pages and replace tabs with spaces.\n",
        "text = \"\"\n",
        "for page in pages:\n",
        "    text += page.page_content\n",
        "text = text.replace('\\t', ' ')\n",
        "\n",
        "# Calculate the number of tokens in the text.\n",
        "num_tokens = llm.get_num_tokens(text)\n",
        "print(f\"This book has {num_tokens} tokens in it\")"
      ],
      "metadata": {
        "id": "vBqbOtxoFREo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into documents using RecursiveCharacterTextSplitter.\n",
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\t\"], chunk_size=10000, chunk_overlap=3000)\n",
        "docs = text_splitter.create_documents([text])\n",
        "\n",
        "# Get the number of documents.\n",
        "num_documents = len(docs)\n",
        "print(f\"Now our book is split up into {num_documents} documents\")"
      ],
      "metadata": {
        "id": "vi-e-fCAFRHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings for the text using OpenAIEmbeddings.\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "vectors = embeddings.embed_documents([x.page_content for x in docs])"
      ],
      "metadata": {
        "id": "20upMKsQFRKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of clusters for K-means clustering.\n",
        "num_clusters = 20\n",
        "\n",
        "# Perform K-means clustering on the embeddings.\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors)"
      ],
      "metadata": {
        "id": "vFZKtR5bFkVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the closest embeddings to the centroids for each cluster.\n",
        "closest_indices = []\n",
        "for i in range(num_clusters):\n",
        "    distances = np.linalg.norm(vectors - kmeans.cluster_centers_[i], axis=1)\n",
        "    closest_index = np.argmin(distances)\n",
        "    closest_indices.append(closest_index)\n",
        "selected_indices = sorted(closest_indices)"
      ],
      "metadata": {
        "id": "yLUv6AhtFkYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ChatOpenAI for text generation.\n",
        "llm3 = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    openai_api_key=openai_api_key,\n",
        "    max_tokens=10000,\n",
        "    model='gpt-3.5-turbo-16k'\n",
        ")"
      ],
      "metadata": {
        "id": "QYa12gmSFkbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt template for summarizing text passages.\n",
        "map_prompt = \"\"\"\n",
        "You will be given a single passage of a book. This section will be enclosed in triple backticks (```)\n",
        "Your goal is to give a summary of this section so that a reader will have a full understanding of what happened.\n",
        "Your response should be at least 20 pages long and fully encompass what was said in the passage.\n",
        "\n",
        "```{text}```\n",
        "FULL SUMMARY:\n",
        "\"\"\"\n",
        "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n",
        "\n",
        "# Load the summarization chain.\n",
        "map_chain = load_summarize_chain(llm=llm3, chain_type=\"stuff\", prompt=map_prompt_template)\n",
        "\n",
        "# Select the documents that were closest to cluster centroids.\n",
        "selected_docs = [docs[doc] for doc in selected_indices]"
      ],
      "metadata": {
        "id": "pWdfiZUvFkd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize a list to store the summaries.\n",
        "summary_list = []\n",
        "\n",
        "# Loop through the selected documents and generate summaries.\n",
        "for i, doc in enumerate(selected_docs):\n",
        "    chunk_summary = map_chain.run([doc])\n",
        "    summary_list.append(chunk_summary)"
      ],
      "metadata": {
        "id": "OMUTxKpQFkgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the FPDF library for creating PDF files.\n",
        "from fpdf import FPDF"
      ],
      "metadata": {
        "id": "H11r6_QAFkjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PDF class instance and set font properties.\n",
        "pdf = FPDF()\n",
        "pdf.set_font(\"Arial\", size=10)\n",
        "\n",
        "# Initialize a page counter.\n",
        "page_counter = 0\n",
        "\n",
        "# Iterate through the summaries and add them to the PDF.\n",
        "for item in summary_list:\n",
        "    if page_counter >= 20:\n",
        "        break  # Exit the loop if the page limit is reached\n",
        "    pdf.add_page()\n",
        "    pdf.multi_cell(0, 10, item)  # Add each item to the PDF\n",
        "    page_counter += 1  # Increment the page counter\n",
        "\n",
        "# Save the PDF to a file.\n",
        "pdf_filename = \"summary_data.pdf\"\n",
        "if page_counter > 20:\n",
        "    pdf.delete_page(21, page_counter)  # Delete excess pages\n",
        "    pdf.output(pdf_filename)\n",
        "else:\n",
        "    pdf.output(pdf_filename)"
      ],
      "metadata": {
        "id": "MYGvUIKnF20x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the summary_list into a single string for token counting.\n",
        "summaries = \"\\n\".join(summary_list)\n",
        "# Convert the summaries back to a document.\n",
        "summaries = Document(page_content=summaries)\n",
        "\n",
        "# Count the number of tokens in the total summary.\n",
        "total_tokens = llm.get_num_tokens(summaries.page_content)\n",
        "print(f\"Your total summary has {total_tokens} tokens\")"
      ],
      "metadata": {
        "id": "-B5W75poF23q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}